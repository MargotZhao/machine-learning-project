{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84c7d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30240bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhaos\\AppData\\Local\\Temp\\ipykernel_11084\\2095908849.py:3: DtypeWarning: Columns (24,25,28,32,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  companies = pd.read_csv('anonymized_hubspot_companies.csv')\n"
     ]
    }
   ],
   "source": [
    "tickets = pd.read_csv('anonymized_hubspot_tickets.csv')\n",
    "deals = pd.read_csv('anonymized_hubspot_deals.csv')\n",
    "companies = pd.read_csv('anonymized_hubspot_companies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Company Size Category ---\n",
    "def size_category(num_employees):\n",
    "    if pd.isnull(num_employees):\n",
    "        return np.nan\n",
    "    elif num_employees < 10:\n",
    "        return \"Very Small\"\n",
    "    elif num_employees < 50:\n",
    "        return \"Small\"\n",
    "    elif num_employees < 250:\n",
    "        return \"Medium\"\n",
    "    elif num_employees < 1000:\n",
    "        return \"Large\"\n",
    "    else:\n",
    "        return \"Enterprise\"\n",
    "\n",
    "companies['Company_Size_Category'] = companies['Number of Employees'].apply(size_category)\n",
    "\n",
    "# --- Revenue Category ---\n",
    "def revenue_category(revenue):\n",
    "    if pd.isnull(revenue):\n",
    "        return np.nan\n",
    "    elif revenue < 1_000_000:\n",
    "        return \"<$1M\"\n",
    "    elif revenue < 10_000_000:\n",
    "        return \"$1M-$10M\"\n",
    "    elif revenue < 50_000_000:\n",
    "        return \"$10M-$50M\"\n",
    "    elif revenue < 100_000_000:\n",
    "        return \"$50M-$100M\"\n",
    "    elif revenue < 500_000_000:\n",
    "        return \"$100M-$500M\"\n",
    "    else:\n",
    "        return \">$500M\"\n",
    "\n",
    "companies['Revenue_Category'] = companies['Annual Revenue'].apply(revenue_category)\n",
    "\n",
    "# --- Industry Standardized ---\n",
    "companies['Industry_Standardized'] = companies['Industry'].str.title()\n",
    "\n",
    "# --- Region ---\n",
    "companies['Region'] = companies['Country/Region'].str.strip()\n",
    "\n",
    "# --- Uses_[Technology] flags ---\n",
    "technology_keywords = ['Google Tag Manager', 'Salesforce', 'Microsoft Office 365']\n",
    "for tech in technology_keywords:\n",
    "    col_name = f'Uses_{tech.replace(\" \", \"_\")}'\n",
    "    companies[col_name] = companies['Web Technologies'].fillna('').apply(lambda x: int(tech.lower() in x.lower()))\n",
    "\n",
    "# --- Technology Count ---\n",
    "companies['Technology_Count'] = companies['Web Technologies'].fillna('').apply(lambda x: len([t for t in x.split(';') if t.strip()]))\n",
    "\n",
    "# --- Is_BPO flag ---\n",
    "companies['Is_BPO'] = companies['BPO'].str.lower().map({'yes': 1, 'no': 0})\n",
    "\n",
    "# --- Date-based Fields ---\n",
    "companies['Create Date'] = pd.to_datetime(companies['Create Date'], errors='coerce')\n",
    "companies['Create_Year'] = companies['Create Date'].dt.year\n",
    "companies['Create_Month'] = companies['Create Date'].dt.month\n",
    "companies['Create_Quarter'] = companies['Create Date'].dt.quarter\n",
    "companies['Create_YearMonth'] = companies['Create Date'].dt.to_period('M').astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "140ec00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up column names just in case\n",
    "tickets.columns = tickets.columns.str.strip()\n",
    "\n",
    "# --- Convert date columns (only those that exist) ---\n",
    "date_cols = [\n",
    "    'Create date', 'Close date', '1st Syms presented for review',\n",
    "    '1st syms run in production',  # This column may still not exist; remove if needed\n",
    "]\n",
    "for col in date_cols:\n",
    "    if col in tickets.columns:\n",
    "        tickets[col] = pd.to_datetime(tickets[col], errors='coerce')\n",
    "\n",
    "# --- Implementation Duration in Days ---\n",
    "tickets['Implementation_Duration_Days'] = (\n",
    "    tickets['Close date'] - tickets['Create date']\n",
    ").dt.days\n",
    "\n",
    "# --- Days to First Sym ---\n",
    "tickets['Days_To_First_Sym'] = (\n",
    "    tickets['1st Syms presented for review'] - tickets['Create date']\n",
    ").dt.days\n",
    "\n",
    "# --- Time to Close in Hours ---\n",
    "def convert_to_hours(x):\n",
    "    try:\n",
    "        h, m, s = map(int, str(x).split(':'))\n",
    "        return h + m/60 + s/3600\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "tickets['Time_To_Close_Hours'] = tickets['Time to close (HH:mm:ss)'].apply(convert_to_hours)\n",
    "\n",
    "# --- Implementation Status (simplified) ---\n",
    "status_map = {\n",
    "    'New': 'Not Started',\n",
    "    'In Progress': 'Ongoing',\n",
    "    'Waiting on contact': 'Ongoing',\n",
    "    'Waiting on us': 'Ongoing',\n",
    "    'Closed': 'Completed'\n",
    "}\n",
    "tickets['Implementation_Status'] = tickets['Ticket status'].map(status_map)\n",
    "\n",
    "# --- Training Completion Count & Percentage ---\n",
    "training_cols = [\n",
    "    'Training: Sym Building 101',\n",
    "    'Training: Sym Building 201',\n",
    "    'Training: General Overview',\n",
    "    'Training: Reporting',\n",
    "    'Training: Deployment/User Management Training'\n",
    "]\n",
    "tickets['Training_Completion_Count'] = tickets[training_cols].notna().sum(axis=1)\n",
    "tickets['Training_Completion_Pct'] = (tickets['Training_Completion_Count'] / len(training_cols)) * 100\n",
    "\n",
    "# --- Create Year, Month, YearMonth ---\n",
    "tickets['Create_Year'] = tickets['Create date'].dt.year\n",
    "tickets['Create_Month'] = tickets['Create date'].dt.month\n",
    "tickets['Create_YearMonth'] = tickets['Create date'].dt.to_period('M').astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "935045c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip column names just in case\n",
    "deals.columns = deals.columns.str.strip()\n",
    "\n",
    "# --- Parse date columns ---\n",
    "deals['Create Date'] = pd.to_datetime(deals['Create Date'], errors='coerce')\n",
    "deals['Close Date'] = pd.to_datetime(deals['Close Date'], errors='coerce')\n",
    "\n",
    "# --- Create Year/Month/Quarter ---\n",
    "deals['Create_Year'] = deals['Create Date'].dt.year\n",
    "deals['Create_Month'] = deals['Create Date'].dt.month\n",
    "deals['Create_Quarter'] = deals['Create Date'].dt.quarter\n",
    "\n",
    "deals['Close_Year'] = deals['Close Date'].dt.year\n",
    "deals['Close_Month'] = deals['Close Date'].dt.month\n",
    "\n",
    "# --- YearMonth string for time-based grouping ---\n",
    "deals['YearMonth'] = deals['Create Date'].dt.to_period('M').astype(str)\n",
    "\n",
    "# --- Deal Size Category ---\n",
    "def deal_size_category(amount):\n",
    "    if pd.isnull(amount):\n",
    "        return None\n",
    "    elif amount < 10000:\n",
    "        return \"Small\"\n",
    "    elif amount < 50000:\n",
    "        return \"Medium\"\n",
    "    elif amount < 250000:\n",
    "        return \"Large\"\n",
    "    else:\n",
    "        return \"Enterprise\"\n",
    "\n",
    "deals['Deal_Size_Category'] = deals['Amount'].apply(deal_size_category)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37523502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with >95% missing\n",
    "def drop_high_na_columns(df, threshold=0.95):\n",
    "    return df.loc[:, df.isnull().mean() <= threshold]\n",
    "\n",
    "companies_cleaned = drop_high_na_columns(companies)\n",
    "tickets_cleaned = drop_high_na_columns(tickets)\n",
    "deals_cleaned = drop_high_na_columns(deals)\n",
    "\n",
    "# Detect outliers using IQR\n",
    "def detect_outliers(df):\n",
    "    outlier_info = {}\n",
    "    for col in df.select_dtypes(include=['number']).columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[col] < lower) | (df[col] > upper)][col]\n",
    "        if not outliers.empty:\n",
    "            outlier_info[col] = {\n",
    "                \"lower_bound\": lower,\n",
    "                \"upper_bound\": upper,\n",
    "                \"num_outliers\": len(outliers),\n",
    "                \"outlier_indices\": outliers.index.tolist()[:5]\n",
    "            }\n",
    "    return outlier_info\n",
    "\n",
    "# Run outlier detection\n",
    "companies_outliers = detect_outliers(companies_cleaned)\n",
    "tickets_outliers = detect_outliers(tickets_cleaned)\n",
    "deals_outliers = detect_outliers(deals_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e122854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Annual Revenue': {'lower_bound': -1493131355.0, 'upper_bound': 2495878813.0, 'num_outliers': 2828, 'outlier_indices': [7, 8, 10, 13, 14]}, 'Number of Form Submissions': {'lower_bound': 0.0, 'upper_bound': 0.0, 'num_outliers': 450, 'outlier_indices': [101, 128, 133, 135, 137]}, 'Number of times contacted': {'lower_bound': -19.5, 'upper_bound': 40.5, 'num_outliers': 1421, 'outlier_indices': [19, 20, 23, 38, 47]}, 'Number of Pageviews': {'lower_bound': 0.0, 'upper_bound': 0.0, 'num_outliers': 427, 'outlier_indices': [101, 128, 133, 135, 139]}, 'Year Founded': {'lower_bound': 1872.5, 'upper_bound': 2084.5, 'num_outliers': 804, 'outlier_indices': [2, 26, 28, 43, 45]}, 'Number of Employees': {'lower_bound': -6875.0, 'upper_bound': 12125.0, 'num_outliers': 2196, 'outlier_indices': [10, 11, 13, 15, 18]}, 'Number of Sessions': {'lower_bound': 0.0, 'upper_bound': 0.0, 'num_outliers': 488, 'outlier_indices': [101, 128, 133, 135, 137]}, 'Technology_Count': {'lower_bound': -4.5, 'upper_bound': 23.5, 'num_outliers': 223, 'outlier_indices': [7, 9, 112, 403, 405]}, 'Create_Year': {'lower_bound': 2023.0, 'upper_bound': 2023.0, 'num_outliers': 6563, 'outlier_indices': [0, 1, 2, 3, 4]}}\n"
     ]
    }
   ],
   "source": [
    "print(companies_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc5fbe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CCaaS', 'Annual Revenue', 'Associated Contact',\n",
      "       'Number of Form Submissions', 'Web Technologies',\n",
      "       'Number of times contacted', 'Contact with Primary Company',\n",
      "       'ICP Fit Level', 'Record ID', 'Time Zone', 'Primary Industry',\n",
      "       'Number of Pageviews', 'Year Founded', 'ICP', 'Segmentation',\n",
      "       'State/Region', 'Consolidated Industry', 'Type', 'Number of Employees',\n",
      "       'Primary Sub-Industry', 'Number of Sessions', 'Country/Region',\n",
      "       'Industry', 'Create Date', 'Company name', 'Last Modified Date',\n",
      "       'Company_Size_Category', 'Revenue_Category', 'Industry_Standardized',\n",
      "       'Region', 'Uses_Google_Tag_Manager', 'Uses_Salesforce',\n",
      "       'Uses_Microsoft_Office_365', 'Technology_Count', 'Create_Year',\n",
      "       'Create_Month', 'Create_Quarter', 'Create_YearMonth'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(companies_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed1448e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the relevant columns to keep\n",
    "columns_to_keep = [\n",
    "    'Record ID', 'Company name', 'Industry', 'Industry_Standardized', 'Consolidated Industry',\n",
    "    'Primary Industry', 'Primary Sub-Industry', 'Country/Region', 'Region', 'State/Region', 'Time Zone',\n",
    "    'Annual Revenue', 'Revenue_Category', 'Number of Employees', 'Company_Size_Category',\n",
    "    'Number of Form Submissions', 'Number of Pageviews', 'Number of Sessions', 'Number of times contacted',\n",
    "    'Technology_Count', 'Uses_Google_Tag_Manager', 'Uses_Salesforce', 'Uses_Microsoft_Office_365',\n",
    "    'Create Date', 'Create_Year', 'Create_Quarter'\n",
    "]\n",
    "\n",
    "# Subset the dataframe\n",
    "companies_segment = companies_cleaned[columns_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3db9c6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Weighted amount', 'Deal Description',\n",
      "       'Cumulative time in \"BANT Deal. Pain ID'ed (Sales Pipeline)\" (HH:mm:ss)',\n",
      "       'Cumulative time in \"Opportunity (Sales Pipeline)\" (HH:mm:ss)',\n",
      "       'Days to close', 'Deal Score', 'Close Date',\n",
      "       'Deal source attribution 2',\n",
      "       'Cumulative time in \"In Trial - Trial in Progress (Sales Pipeline)\" (HH:mm:ss)',\n",
      "       'Contract Start Date',\n",
      "       'Cumulative time in \"Partner Referrals  (Sales Pipeline)\" (HH:mm:ss)',\n",
      "       'Cumulative time in \"Deep Dive. PSP Drafted (Sales Pipeline)\" (HH:mm:ss)',\n",
      "       'Pipeline', 'Record ID', 'Forecast category', 'Original Traffic Source',\n",
      "       'Associated Company', 'Deal owner', 'Is Closed (numeric)',\n",
      "       'Amount in company currency', 'Deal probability',\n",
      "       'Associated Company (Primary)', 'Is Closed Won', 'Contract End Date',\n",
      "       'Last Activity Date', 'Contract Term (Months)', 'Trial Start date',\n",
      "       'Is closed lost', 'Weighted amount in company currency',\n",
      "       'Is Deal Closed?', 'Trial End Date', 'Deal Name', 'Amount',\n",
      "       'Forecast amount',\n",
      "       'Cumulative time in \"Negotiation (Sales Pipeline)\" (HH:mm:ss)',\n",
      "       'Is Open (numeric)',\n",
      "       'Cumulative time in \"Renewals  (Sales Pipeline)\" (HH:mm:ss)',\n",
      "       'Create Date',\n",
      "       'Cumulative time in \"Contract Sent (Sales Pipeline)\" (HH:mm:ss)',\n",
      "       'Last Modified Date', 'Deal Stage', 'Deal Type', 'Create_Year',\n",
      "       'Create_Month', 'Create_Quarter', 'Close_Year', 'Close_Month',\n",
      "       'YearMonth', 'Deal_Size_Category'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(deals_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b158b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant columns for segmentation and deal prediction\n",
    "columns_to_keep = [\n",
    "    'Record ID', 'Deal Name', 'Associated Company', 'Associated Company (Primary)',\n",
    "    'Amount', 'Weighted amount', 'Forecast amount', 'Amount in company currency',\n",
    "    'Deal Score', 'Deal probability', 'Forecast category', 'Pipeline', 'Deal Stage',\n",
    "    'Deal Type', 'Is Closed (numeric)', 'Is closed lost', 'Is Closed Won', 'Is Deal Closed?',\n",
    "    'Is Open (numeric)', 'Contract Term (Months)', 'Days to close', 'Create Date',\n",
    "    'Close Date', 'Create_Year', 'Create_Quarter', 'Close_Year', 'Deal_Size_Category',\n",
    "    'Original Traffic Source', 'Deal source attribution 2'\n",
    "]\n",
    "\n",
    "# Subset the deals dataframe\n",
    "deals_segment = deals_cleaned[columns_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61120688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Create date', 'Stage Date - Project Launch', 'Associated Contact',\n",
      "       'Target Launch Date', 'Kickoff Call', 'Close date', 'Pipeline',\n",
      "       'Stage Date - Project Initiation', 'Ticket status',\n",
      "       'Stage Date - Execution', 'Time to first agent email reply (HH:mm:ss)',\n",
      "       'Associated Company', 'Stage Date - Closure Phase',\n",
      "       'Associated Company (Primary)', 'Response time (HH:mm:ss)', 'Ticket ID',\n",
      "       'Time to close (HH:mm:ss)', 'Associated Deal', 'Library index approved',\n",
      "       'Training: General Overview', 'Latest Milestone',\n",
      "       'Training: Deployment/User Management Training',\n",
      "       'Requirements for the Trial', 'Training: Sym Building 101',\n",
      "       'Last modified date', 'Trial End Date', 'Training: Sym Building 201',\n",
      "       'Stage Date - Converted Won', 'Latest Milestone Update Date',\n",
      "       'Ticket name', 'Trial Overview', 'Trial Start Date',\n",
      "       '1st Syms presented for review', 'Project Launch Day',\n",
      "       'Training: Reporting', 'Construction of 1st Sym begun',\n",
      "       'Implementation_Duration_Days', 'Days_To_First_Sym',\n",
      "       'Time_To_Close_Hours', 'Training_Completion_Count',\n",
      "       'Training_Completion_Pct', 'Create_Year', 'Create_Month',\n",
      "       'Create_YearMonth'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(tickets_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d3dc193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant columns for segmentation and prediction\n",
    "columns_to_keep = [\n",
    "    'Ticket ID', 'Ticket name', 'Associated Company', 'Associated Company (Primary)', 'Associated Deal',\n",
    "    'Create date', 'Close date', 'Implementation_Duration_Days', 'Days_To_First_Sym', 'Time_To_Close_Hours',\n",
    "    'Create_Year', 'Create_Month', 'Create_YearMonth',\n",
    "    'Ticket status', 'Pipeline', 'Latest Milestone',\n",
    "    'Trial Start Date', 'Trial End Date',\n",
    "    'Training_Completion_Count', 'Training_Completion_Pct',\n",
    "    'Time to first agent email reply (HH:mm:ss)', 'Response time (HH:mm:ss)'\n",
    "]\n",
    "\n",
    "# Subset the tickets dataframe\n",
    "tickets_segment = tickets_cleaned[columns_to_keep]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb32f8",
   "metadata": {},
   "source": [
    "### Data cleaning for companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8cf96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further data cleaning on companies\n",
    "# 1. Drop rows with missing key segmentation fields\n",
    "companies_cleaned = companies_cleaned.dropna(subset=['Industry', 'Annual Revenue', 'Number of Employees'])\n",
    "\n",
    "# 2. Standardize Country/Region\n",
    "companies_cleaned['Country/Region'] = companies_cleaned['Country/Region'].astype(str).str.strip().str.title()\n",
    "\n",
    "# 3. Clean Web Technologies field\n",
    "def clean_web_tech(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    return [tech.strip() for tech in text.split(';') if tech.strip()]\n",
    "\n",
    "companies_cleaned['Web Technologies'] = companies_cleaned['Web Technologies'].apply(clean_web_tech)\n",
    "\n",
    "# 4. Drop parent/child company relationship fields if present and mostly empty\n",
    "cols_to_drop = ['Parent Company', 'Associated Company']\n",
    "companies_cleaned = companies_cleaned.drop(columns=[col for col in cols_to_drop if col in companies_cleaned.columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a823cfdd",
   "metadata": {},
   "source": [
    "### Data cleaning for tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef7e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert time columns to seconds\n",
    "def time_to_seconds(x):\n",
    "    try:\n",
    "        h, m, s = map(int, str(x).split(\":\"))\n",
    "        return h * 3600 + m * 60 + s\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "time_columns = ['Time to close (HH:mm:ss)', 'Response time (HH:mm:ss)', 'Time to first agent email reply (HH:mm:ss)']\n",
    "for col in time_columns:\n",
    "    if col in tickets_cleaned.columns:\n",
    "        tickets_cleaned[col + '_seconds'] = tickets_cleaned[col].apply(time_to_seconds)\n",
    "\n",
    "# 2. Training field completeness (flag if at least one completed)\n",
    "training_cols = [\n",
    "    'Training: Sym Building 101',\n",
    "    'Training: Sym Building 201',\n",
    "    'Training: General Overview',\n",
    "    'Training: Reporting',\n",
    "    'Training: Deployment/User Management Training'\n",
    "]\n",
    "for col in training_cols:\n",
    "    if col in tickets_cleaned.columns:\n",
    "        tickets_cleaned[col + '_Completed'] = tickets_cleaned[col].notna().astype(int)\n",
    "\n",
    "# 3. Flag for ongoing implementation (missing milestone allowed)\n",
    "tickets_cleaned['Is_Closed'] = tickets_cleaned['Ticket status'].str.lower() == 'closed'\n",
    "\n",
    "# 4. Flag for whether ticket is linked to a deal\n",
    "tickets_cleaned['Has_Associated_Deal'] = tickets_cleaned['Associated Deal'].notna().astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389636f",
   "metadata": {},
   "source": [
    "### Data cleaning for deals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98129b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Handle missing Amount values\n",
    "# Option 1: Drop rows where 'Amount' is missing\n",
    "deals_cleaned = deals_cleaned[deals_cleaned['Amount'].notna()]\n",
    "\n",
    "# Option 2 (alternative): Impute with median by Deal Type\n",
    "# deals_cleaned['Amount'] = deals_cleaned.groupby('Deal Type')['Amount'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# 2. Remove outliers in Days to close using IQR\n",
    "def remove_outliers_iqr(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return df[(df[col] >= lower) & (df[col] <= upper)]\n",
    "\n",
    "if 'Days to close' in deals_cleaned.columns:\n",
    "    deals_cleaned = remove_outliers_iqr(deals_cleaned, 'Days to close')\n",
    "\n",
    "# 3. Optional: Bin Deal probability into categories\n",
    "def bin_probability(p):\n",
    "    if pd.isna(p):\n",
    "        return 'Unknown'\n",
    "    elif p < 0.3:\n",
    "        return 'Low'\n",
    "    elif p < 0.7:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "deals_cleaned['Deal_Probability_Bin'] = deals_cleaned['Deal probability'].apply(bin_probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e44ebd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tickets with non-null 'Associated Deal': 79\n",
      "Matching deals found in deals_cleaned dataset: 27\n"
     ]
    }
   ],
   "source": [
    "# Ensure deal identifier fields are strings\n",
    "tickets_cleaned.loc[:, 'Associated Deal'] = tickets_cleaned['Associated Deal'].astype(str)\n",
    "deals_cleaned.loc[:, 'Deal Name'] = deals_cleaned['Deal Name'].astype(str)\n",
    "\n",
    "# Find tickets whose 'Associated Deal' matches a 'Deal Name'\n",
    "overlapping_deals = tickets_cleaned[tickets_cleaned['Associated Deal'].isin(deals_cleaned['Deal Name'])]\n",
    "\n",
    "# Summary stats\n",
    "overlap_count = overlapping_deals['Associated Deal'].nunique()\n",
    "total_tickets_with_deals = tickets_cleaned['Associated Deal'].notnull().sum()\n",
    "\n",
    "print(\"Total tickets with non-null 'Associated Deal':\", total_tickets_with_deals)\n",
    "print(\"Matching deals found in deals_cleaned dataset:\", overlap_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888dbc6",
   "metadata": {},
   "source": [
    "### Merge tickets and deals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58649d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merge Summary:\n",
      "Tickets with mapped deal: 58\n",
      "Merged Rows: 43\n",
      "Unique Ticket IDs in Merge: 42\n",
      "Unique Deal Names in Merge: 27\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: Load mapping file (CompanyToTickets)\n",
    "with open('mappings.json', 'r') as f:\n",
    "    mappings = json.load(f)\n",
    "\n",
    "# Load mappings.json\n",
    "with open(\"mappings.json\", \"r\") as f:\n",
    "    mappings = json.load(f)\n",
    "\n",
    "# Extract TicketToDeal mapping\n",
    "ticket_to_deal = mappings[\"TicketToDeal\"]\n",
    "\n",
    "# Map ticket IDs to Deal Names\n",
    "tickets_cleaned.loc[:, \"Ticket ID\"] = tickets_cleaned[\"Ticket ID\"].astype(str)\n",
    "deals_cleaned.loc[:, \"Deal Name\"] = deals_cleaned[\"Deal Name\"].astype(str)\n",
    "tickets_cleaned.loc[:, \"Mapped Deal Name\"] = tickets_cleaned[\"Ticket ID\"].map(ticket_to_deal)\n",
    "\n",
    "# Merge using Deal Name (not Record ID!)\n",
    "merged_tickets_deals = pd.merge(\n",
    "    tickets_cleaned,\n",
    "    deals_cleaned,\n",
    "    left_on=\"Mapped Deal Name\",\n",
    "    right_on=\"Deal Name\",\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_ticket\", \"_deal\")\n",
    ")\n",
    "\n",
    "# Optional: Summary of the merge\n",
    "merge_report = {\n",
    "    \"Tickets with mapped deal\": tickets_cleaned[\"Mapped Deal Name\"].notnull().sum(),\n",
    "    \"Merged Rows\": merged_tickets_deals.shape[0],\n",
    "    \"Unique Ticket IDs in Merge\": merged_tickets_deals[\"Ticket ID\"].nunique(),\n",
    "    \"Unique Deal Names in Merge\": merged_tickets_deals[\"Deal Name\"].nunique()\n",
    "}\n",
    "\n",
    "print(\"✅ Merge Summary:\")\n",
    "for k, v in merge_report.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49a71d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Merge Quality Report\n",
      "Tickets with mapped deals: 58\n",
      "Merged rows: 43\n",
      "Unique ticket IDs in merged data: 42\n",
      "Unique deal names in merged data: 27\n",
      "Duplicate ticket IDs in merged data: 1\n",
      "Deals linked to multiple tickets: 9\n",
      "Closed Won deals (if available): 26\n"
     ]
    }
   ],
   "source": [
    "# Check the merge quality\n",
    "# Check total number of tickets that had a mapping\n",
    "total_mapped_tickets = tickets_cleaned[\"Mapped Deal Name\"].notnull().sum()\n",
    "\n",
    "# Check how many tickets successfully merged\n",
    "total_merged_rows = merged_tickets_deals.shape[0]\n",
    "unique_ticket_ids = merged_tickets_deals[\"Ticket ID\"].nunique()\n",
    "unique_deal_names = merged_tickets_deals[\"Deal Name\"].nunique()\n",
    "\n",
    "# Check for duplicates (1 ticket → multiple deal rows)\n",
    "duplicate_ticket_ids = merged_tickets_deals[\"Ticket ID\"].duplicated().sum()\n",
    "\n",
    "# Check for deals associated with multiple tickets\n",
    "deal_to_ticket_counts = merged_tickets_deals[\"Mapped Deal Name\"].value_counts()\n",
    "deals_with_multiple_tickets = (deal_to_ticket_counts > 1).sum()\n",
    "\n",
    "# Optional: Check how many merged deals are Closed Won\n",
    "if \"Is Closed Won\" in merged_tickets_deals.columns:\n",
    "    closed_won_count = merged_tickets_deals[\"Is Closed Won\"].sum()\n",
    "else:\n",
    "    closed_won_count = \"Not Available\"\n",
    "\n",
    "# Compile report\n",
    "merge_quality_report = {\n",
    "    \"Tickets with mapped deals\": total_mapped_tickets,\n",
    "    \"Merged rows\": total_merged_rows,\n",
    "    \"Unique ticket IDs in merged data\": unique_ticket_ids,\n",
    "    \"Unique deal names in merged data\": unique_deal_names,\n",
    "    \"Duplicate ticket IDs in merged data\": duplicate_ticket_ids,\n",
    "    \"Deals linked to multiple tickets\": deals_with_multiple_tickets,\n",
    "    \"Closed Won deals (if available)\": closed_won_count\n",
    "}\n",
    "\n",
    "# Display report\n",
    "print(\"📊 Merge Quality Report\")\n",
    "for key, value in merge_quality_report.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86fa8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure keys are strings and merge\n",
    "merged_tickets_deals['Associated Company (Primary)_deal'] = merged_tickets_deals['Associated Company (Primary)_deal'].astype(str).str.strip()\n",
    "companies_cleaned['Company name'] = companies_cleaned['Company name'].astype(str).str.strip()\n",
    "\n",
    "merged_full = pd.merge(\n",
    "    merged_tickets_deals,\n",
    "    companies_cleaned,\n",
    "    left_on='Associated Company (Primary)_deal',\n",
    "    right_on='Company name',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 2: Select relevant features for segmentation and value analysis\n",
    "segmentation_cols = [\n",
    "    'Company name', 'Industry_Standardized', 'Region', 'Revenue_Category',\n",
    "    'Company_Size_Category', 'Technology_Count', 'Uses_Google_Tag_Manager',\n",
    "    'Uses_Salesforce', 'Uses_Microsoft_Office_365', 'Annual Revenue',\n",
    "    'Number of Employees', 'Amount', 'Forecast amount', 'Deal Score',\n",
    "    'Deal probability', 'Pipeline_ticket', 'Ticket status', 'Training_Completion_Pct',\n",
    "    'Implementation_Duration_Days', 'Time_To_Close_Hours'\n",
    "]\n",
    "\n",
    "# Step 3: Clean the subset\n",
    "segmentation_data = merged_full[segmentation_cols].dropna()\n",
    "\n",
    "# Step 4: Optional — convert categorical vars to category dtype (for modeling or clustering)\n",
    "categorical_cols = [\n",
    "    'Industry_Standardized', 'Region', 'Revenue_Category',\n",
    "    'Company_Size_Category', 'Pipeline_ticket', 'Ticket status'\n",
    "]\n",
    "for col in categorical_cols:\n",
    "    segmentation_data[col] = segmentation_data[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c92a6193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Create date', 'Stage Date - Project Launch', 'Associated Contact', 'Target Launch Date', 'Kickoff Call', 'Close date', 'Pipeline_ticket', 'Stage Date - Project Initiation', 'Ticket status', 'Stage Date - Execution', 'Time to first agent email reply (HH:mm:ss)', 'Associated Company_ticket', 'Stage Date - Closure Phase', 'Associated Company (Primary)_ticket', 'Response time (HH:mm:ss)', 'Ticket ID', 'Time to close (HH:mm:ss)', 'Associated Deal', 'Library index approved', 'Training: General Overview', 'Latest Milestone', 'Training: Deployment/User Management Training', 'Requirements for the Trial', 'Training: Sym Building 101', 'Last modified date', 'Trial End Date_ticket', 'Training: Sym Building 201', 'Stage Date - Converted Won', 'Latest Milestone Update Date', 'Ticket name', 'Trial Overview', 'Trial Start Date', '1st Syms presented for review', 'Project Launch Day', 'Training: Reporting', 'Construction of 1st Sym begun', 'Implementation_Duration_Days', 'Days_To_First_Sym', 'Time_To_Close_Hours', 'Training_Completion_Count', 'Training_Completion_Pct', 'Create_Year_ticket', 'Create_Month_ticket', 'Create_YearMonth', 'Mapped Deal Name', 'Weighted amount', 'Deal Description', 'Cumulative time in \"BANT Deal. Pain ID\\'ed (Sales Pipeline)\" (HH:mm:ss)', 'Cumulative time in \"Opportunity (Sales Pipeline)\" (HH:mm:ss)', 'Days to close', 'Deal Score', 'Close Date', 'Deal source attribution 2', 'Cumulative time in \"In Trial - Trial in Progress (Sales Pipeline)\" (HH:mm:ss)', 'Contract Start Date', 'Cumulative time in \"Partner Referrals  (Sales Pipeline)\" (HH:mm:ss)', 'Cumulative time in \"Deep Dive. PSP Drafted (Sales Pipeline)\" (HH:mm:ss)', 'Pipeline_deal', 'Record ID', 'Forecast category', 'Original Traffic Source', 'Associated Company_deal', 'Deal owner', 'Is Closed (numeric)', 'Amount in company currency', 'Deal probability', 'Associated Company (Primary)_deal', 'Is Closed Won', 'Contract End Date', 'Last Activity Date', 'Contract Term (Months)', 'Trial Start date', 'Is closed lost', 'Weighted amount in company currency', 'Is Deal Closed?', 'Trial End Date_deal', 'Deal Name', 'Amount', 'Forecast amount', 'Cumulative time in \"Negotiation (Sales Pipeline)\" (HH:mm:ss)', 'Is Open (numeric)', 'Cumulative time in \"Renewals  (Sales Pipeline)\" (HH:mm:ss)', 'Create Date', 'Cumulative time in \"Contract Sent (Sales Pipeline)\" (HH:mm:ss)', 'Last Modified Date', 'Deal Stage', 'Deal Type', 'Create_Year_deal', 'Create_Month_deal', 'Create_Quarter', 'Close_Year', 'Close_Month', 'YearMonth', 'Deal_Size_Category', 'Deal_Probability_Bin']\n"
     ]
    }
   ],
   "source": [
    "print(merged_tickets_deals.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad91a815",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# STEP 1: Merge (adjust these to match your variable names)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\matplotlib\\__init__.py:1270\u001b[0m\n\u001b[0;32m   1266\u001b[0m     rcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend_fallback\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMPLBACKEND\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m-> 1270\u001b[0m     \u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMPLBACKEND\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_backend\u001b[39m():\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;124;03m    Return the name of the current backend.\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    matplotlib.use\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\matplotlib\\__init__.py:736\u001b[0m, in \u001b[0;36mRcParams.__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m    734\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 736\u001b[0m     cval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\matplotlib\\rcsetup.py:273\u001b[0m, in \u001b[0;36mvalidate_backend\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_backend\u001b[39m(s):\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m _auto_backend_sentinel \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mbackend_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_valid_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\matplotlib\\backends\\registry.py:250\u001b[0m, in \u001b[0;36mBackendRegistry.is_valid_backend\u001b[1;34m(self, backend)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Only load entry points if really need to and not already done so.\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_entry_points_loaded\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend_to_gui_framework:\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\matplotlib\\backends\\registry.py:116\u001b[0m, in \u001b[0;36mBackendRegistry._ensure_entry_points_loaded\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_entry_points_loaded\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# Load entry points, if they have not already been loaded.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_entry_points:\n\u001b[1;32m--> 116\u001b[0m         entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_entry_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_store_entry_points(entries)\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_entry_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\matplotlib\\backends\\registry.py:140\u001b[0m, in \u001b[0;36mBackendRegistry._read_entry_points\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    138\u001b[0m group \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatplotlib.backend\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m--> 140\u001b[0m     entry_points \u001b[38;5;241m=\u001b[39m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m     entry_points \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mentry_points()\u001b[38;5;241m.\u001b[39mget(group, ())\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\metadata\\__init__.py:913\u001b[0m, in \u001b[0;36mentry_points\u001b[1;34m(**params)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return EntryPoint objects for all installed packages.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03mPass selection parameters (group or name) to filter the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;124;03m:return: EntryPoints for all installed packages.\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    910\u001b[0m eps \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m    911\u001b[0m     dist\u001b[38;5;241m.\u001b[39mentry_points \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m _unique(distributions())\n\u001b[0;32m    912\u001b[0m )\n\u001b[1;32m--> 913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEntryPoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\metadata\\__init__.py:911\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mentry_points\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EntryPoints:\n\u001b[0;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return EntryPoint objects for all installed packages.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    Pass selection parameters (group or name) to filter the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;124;03m    :return: EntryPoints for all installed packages.\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    910\u001b[0m     eps \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m--> 911\u001b[0m         \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_points\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m _unique(distributions())\n\u001b[0;32m    912\u001b[0m     )\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EntryPoints(eps)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\metadata\\__init__.py:471\u001b[0m, in \u001b[0;36mDistribution.entry_points\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mentry_points\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EntryPoints\u001b[38;5;241m.\u001b[39m_from_text_for(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentry_points.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\metadata\\__init__.py:819\u001b[0m, in \u001b[0;36mPathDistribution.read_text\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m,\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;167;01mPermissionError\u001b[39;00m,\n\u001b[0;32m    818\u001b[0m     ):\n\u001b[1;32m--> 819\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoinpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\pathlib.py:1027\u001b[0m, in \u001b[0;36mPath.read_text\u001b[1;34m(self, encoding, errors)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;124;03mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1027\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\pathlib.py:1013\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1012\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen codecs>:309\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# STEP 1: Merge (adjust these to match your variable names)\n",
    "merged_tickets_deals['Associated Company (Primary)_deal'] = merged_tickets_deals['Associated Company (Primary)_deal'].astype(str).str.strip()\n",
    "companies_cleaned['Company name'] = companies_cleaned['Company name'].astype(str).str.strip()\n",
    "\n",
    "merged_full = pd.merge(\n",
    "    merged_tickets_deals,\n",
    "    companies_cleaned,\n",
    "    left_on='Associated Company (Primary)_deal',\n",
    "    right_on='Company name',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# STEP 2: Select segmentation features\n",
    "segmentation_cols = [\n",
    "    'Company name', 'Industry_Standardized', 'Region', 'Revenue_Category',\n",
    "    'Company_Size_Category', 'Technology_Count', 'Uses_Google_Tag_Manager',\n",
    "    'Uses_Salesforce', 'Uses_Microsoft_Office_365', 'Annual Revenue',\n",
    "    'Number of Employees', 'Amount', 'Forecast amount', 'Deal Score',\n",
    "    'Deal probability', 'Pipeline_ticket', 'Ticket status', 'Training_Completion_Pct',\n",
    "    'Implementation_Duration_Days', 'Time_To_Close_Hours'\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    'Industry_Standardized', 'Region', 'Revenue_Category',\n",
    "    'Company_Size_Category', 'Pipeline_ticket', 'Ticket status'\n",
    "]\n",
    "\n",
    "# STEP 3: Drop missing values\n",
    "segmentation_data = merged_full[segmentation_cols].dropna().copy()\n",
    "\n",
    "# STEP 4: Encode categorical columns\n",
    "segmentation_encoded = pd.get_dummies(segmentation_data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# STEP 5: Normalize numeric columns\n",
    "numeric_cols = segmentation_encoded.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = StandardScaler()\n",
    "segmentation_encoded[numeric_cols] = scaler.fit_transform(segmentation_encoded[numeric_cols])\n",
    "\n",
    "# STEP 6: Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "segmentation_encoded['Cluster'] = kmeans.fit_predict(segmentation_encoded)\n",
    "\n",
    "# STEP 7: Attach cluster back to original\n",
    "segmentation_data['Cluster'] = segmentation_encoded['Cluster']\n",
    "\n",
    "# STEP 8: Summary insights\n",
    "summary = segmentation_data.groupby('Cluster')[['Annual Revenue', 'Amount', 'Deal Score']].mean().round(2)\n",
    "\n",
    "# STEP 9: Plot average annual revenue per cluster\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=summary.reset_index(), x='Cluster', y='Annual Revenue')\n",
    "plt.title('Average Annual Revenue by Cluster')\n",
    "plt.ylabel('Annual Revenue')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c68c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
